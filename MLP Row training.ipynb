{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34925d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import  StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ae6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('RowFinal1.csv')\n",
    "A=df.drop('Label',axis=1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(A, df.Label, test_size=0.3, random_state=4)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a910011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=300,activation = 'relu'\n",
    "                    ,solver='adam',random_state=23)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred=mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b258ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm\n",
    "\n",
    "m2 = cm(y_test,y_pred)\n",
    "print(m2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(m2, annot=True, fmt='g', ax=ax); \n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix for RowFinal1'); \n",
    "ax.xaxis.set_ticklabels(['Healthy', 'Demented']); ax.yaxis.set_ticklabels(['Healthy', 'Demented']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eeaed667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.77      0.75      1714\n",
      "         1.0       0.76      0.72      0.74      1727\n",
      "\n",
      "    accuracy                           0.74      3441\n",
      "   macro avg       0.74      0.74      0.74      3441\n",
      "weighted avg       0.74      0.74      0.74      3441\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.73      0.73      1714\n",
      "         1.0       0.73      0.75      0.74      1727\n",
      "\n",
      "    accuracy                           0.74      3441\n",
      "   macro avg       0.74      0.74      0.74      3441\n",
      "weighted avg       0.74      0.74      0.74      3441\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.75      0.73      1714\n",
      "         1.0       0.74      0.70      0.72      1727\n",
      "\n",
      "    accuracy                           0.73      3441\n",
      "   macro avg       0.73      0.73      0.73      3441\n",
      "weighted avg       0.73      0.73      0.73      3441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.70      0.71      1715\n",
      "         1.0       0.71      0.74      0.73      1726\n",
      "\n",
      "    accuracy                           0.72      3441\n",
      "   macro avg       0.72      0.72      0.72      3441\n",
      "weighted avg       0.72      0.72      0.72      3441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.74      0.73      1715\n",
      "         1.0       0.74      0.73      0.74      1726\n",
      "\n",
      "    accuracy                           0.73      3441\n",
      "   macro avg       0.73      0.73      0.73      3441\n",
      "weighted avg       0.73      0.73      0.73      3441\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.79      0.75      1715\n",
      "         1.0       0.77      0.68      0.72      1726\n",
      "\n",
      "    accuracy                           0.74      3441\n",
      "   macro avg       0.74      0.74      0.73      3441\n",
      "weighted avg       0.74      0.74      0.73      3441\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.73      0.74      1714\n",
      "         1.0       0.74      0.75      0.74      1726\n",
      "\n",
      "    accuracy                           0.74      3440\n",
      "   macro avg       0.74      0.74      0.74      3440\n",
      "weighted avg       0.74      0.74      0.74      3440\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.72      0.73      1714\n",
      "         1.0       0.73      0.74      0.74      1726\n",
      "\n",
      "    accuracy                           0.73      3440\n",
      "   macro avg       0.73      0.73      0.73      3440\n",
      "weighted avg       0.73      0.73      0.73      3440\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.71      0.72      1714\n",
      "         1.0       0.72      0.75      0.74      1726\n",
      "\n",
      "    accuracy                           0.73      3440\n",
      "   macro avg       0.73      0.73      0.73      3440\n",
      "weighted avg       0.73      0.73      0.73      3440\n",
      "\n",
      "\n",
      "\n",
      "Cross validation metrics :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.75      0.74      1714\n",
      "         1.0       0.75      0.74      0.74      1726\n",
      "\n",
      "    accuracy                           0.74      3440\n",
      "   macro avg       0.74      0.74      0.74      3440\n",
      "weighted avg       0.74      0.74      0.74      3440\n",
      "\n",
      "Cross Validation scores: [0.74396978 0.7378669  0.72595176 0.72101133 0.73496077 0.73554199\n",
      " 0.73924419 0.73226744 0.73023256 0.74418605]\n",
      "\n",
      "\n",
      "Cross Validation accuracy: 0.735 +/- 0.007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# cv=10 represents the StratifiedKFold with 10 folds\n",
    "x1=[];\n",
    "def cls_rep(y_true,y_pred):\n",
    "    \n",
    "    print ('\\n\\nCross validation metrics :\\n'\n",
    "           \n",
    "           \n",
    "           ,classification_report(y_true, y_pred))\n",
    "    x1.append(classification_report(y_true, y_pred, output_dict = True, digits = 2))\n",
    "    \n",
    "    return accuracy_score(y_true,y_pred)\n",
    "\n",
    "\n",
    "scores1 = cross_val_score(mlp, X=X_train, y=y_train, cv=10, n_jobs=1, scoring = make_scorer(cls_rep))\n",
    "\n",
    " \n",
    "print('Cross Validation scores: %s' % scores1)\n",
    " \n",
    "print('\\n\\nCross Validation accuracy: %.3f +/- %.3f' % (np.mean(scores1),np.std(scores1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f5c26b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     16\u001b[0m     fin \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(fin,index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m(fin)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m((parse(x1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def round_dict(d,k):\n",
    "    res = dict()\n",
    "    for key in d:\n",
    "        res[key]=round(d[key],k)\n",
    "    return res\n",
    "        \n",
    "\n",
    "\n",
    "def parse(d):\n",
    "    fin = dict()\n",
    "    i=-1\n",
    "    k=3\n",
    "    for key in d:\n",
    "        i=i+1\n",
    "        fin[i]=round_dict(key,k)\n",
    "    fin = pd.DataFrame(fin,index= None)\n",
    "    return(fin)\n",
    " \n",
    "\n",
    "    \n",
    "print(x1['0'])\n",
    "print((parse(x1['0'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba81a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2)\n"
     ]
    }
   ],
   "source": [
    "x1 = pd.DataFrame(x1,index=None)\n",
    "\n",
    "f1 = [\"Label:0\",parse(x1['0.0'])]\n",
    "f2 = [\"Label:1\",parse(x1['1.0'])]\n",
    "f3 = [\"Accuracy\",(x1['accuracy'])]\n",
    "f4 = [\"Macro Avg\",parse(x1['macro avg'])]\n",
    "f5 = [\"Weighted Avg\",parse(x1['weighted avg'])]\n",
    "f6 = [\"Avg Accuracy\",'%.3f +/- %.3f' % (np.mean(scores1),np.std(scores1))]\n",
    "f = (f1,f2,f3,f4,f5,f6)\n",
    "\n",
    "f = pd.DataFrame(f, columns = ['Types','Rows'], index = None)\n",
    "\n",
    "print(f.shape)\n",
    "f.to_csv('DTRows.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
